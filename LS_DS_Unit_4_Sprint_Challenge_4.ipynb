{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "LS_DS_Unit_4_Sprint_Challenge_4.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Higgins2718/DS-Unit-4-Sprint-3-Deep-Learning/blob/master/LS_DS_Unit_4_Sprint_Challenge_4.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9MXwS7fX5kB-",
        "colab_type": "text"
      },
      "source": [
        "<img align=\"left\" src=\"https://lever-client-logos.s3.amazonaws.com/864372b1-534c-480e-acd5-9711f850815c-1524247202159.png\" width=200>\n",
        "<br></br>\n",
        "<br></br>\n",
        "\n",
        "# Major Neural Network Architectures Challenge\n",
        "## *Data Science Unit 4 Sprint 3 Challenge*\n",
        "\n",
        "In this sprint challenge, you'll explore some of the cutting edge of Data Science. This week we studied several famous neural network architectures: \n",
        "recurrent neural networks (RNNs), long short-term memory (LSTMs), convolutional neural networks (CNNs), and Generative Adverserial Networks (GANs). In this sprint challenge, you will revisit these models. Remember, we are testing your knowledge of these architectures not your ability to fit a model with high accuracy. \n",
        "\n",
        "__*Caution:*__  these approaches can be pretty heavy computationally. All problems were designed so that you should be able to achieve results within at most 5-10 minutes of runtime on Colab or a comparable environment. If something is running longer, doublecheck your approach!\n",
        "\n",
        "## Challenge Objectives\n",
        "*You should be able to:*\n",
        "* <a href=\"#p1\">Part 1</a>: Train a RNN classification model\n",
        "* <a href=\"#p2\">Part 2</a>: Utilize a pre-trained CNN for objective detection\n",
        "* <a href=\"#p3\">Part 3</a>: Describe the difference between a discriminator and generator in a GAN\n",
        "* <a href=\"#p4\">Part 4</a>: Describe yourself as a Data Science and elucidate your vision of AI"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "-5UwGRnJOmD4"
      },
      "source": [
        "<a id=\"p1\"></a>\n",
        "## Part 1 - RNNs\n",
        "\n",
        "Use an RNN to fit a multi-class classification model on reuters news articles to distinguish topics of articles. The data is already encoded properly for use in an RNN model. \n",
        "\n",
        "Your Tasks: \n",
        "- Use Keras to fit a predictive model, classifying news articles into topics. \n",
        "- Report your overall score and accuracy\n",
        "\n",
        "For reference, the [Keras IMDB sentiment classification example](https://github.com/keras-team/keras/blob/master/examples/imdb_lstm.py) will be useful, as well the RNN code we used in class.\n",
        "\n",
        "__*Note:*__  Focus on getting a running model, not on maxing accuracy with extreme data size or epoch numbers. Only revisit and push accuracy if you get everything else done!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p6wMLF4a7Kf6",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 308
        },
        "outputId": "df308d8a-0b6c-4324-b313-e25a93cf94d2"
      },
      "source": [
        "!pip install numpy==1.16.1"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting numpy==1.16.1\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/f5/bf/4981bcbee43934f0adb8f764a1e70ab0ee5a448f6505bd04a87a2fda2a8b/numpy-1.16.1-cp36-cp36m-manylinux1_x86_64.whl (17.3MB)\n",
            "\u001b[K     |████████████████████████████████| 17.3MB 2.7MB/s \n",
            "\u001b[31mERROR: datascience 0.10.6 has requirement folium==0.2.1, but you'll have folium 0.8.3 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: albumentations 0.1.12 has requirement imgaug<0.2.7,>=0.2.5, but you'll have imgaug 0.2.9 which is incompatible.\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: numpy\n",
            "  Found existing installation: numpy 1.16.4\n",
            "    Uninstalling numpy-1.16.4:\n",
            "      Successfully uninstalled numpy-1.16.4\n",
            "Successfully installed numpy-1.16.1\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "numpy"
                ]
              }
            }
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "DS-9ksWjoJit",
        "colab": {}
      },
      "source": [
        "from tensorflow.keras.datasets import reuters\n",
        "\n",
        "(x_train, y_train), (x_test, y_test) = reuters.load_data(num_words=None,\n",
        "                                                         skip_top=0,\n",
        "                                                         maxlen=None,\n",
        "                                                         test_split=0.2,\n",
        "                                                         seed=723812,\n",
        "                                                         start_char=1,\n",
        "                                                         oov_char=2,\n",
        "                                                         index_from=3)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "fLKqFh8DovaN",
        "outputId": "40e11a16-1e0b-4d23-aeeb-29b91f81dc27",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        }
      },
      "source": [
        "# Demo of encoding\n",
        "\n",
        "word_index = reuters.get_word_index(path=\"reuters_word_index.json\")\n",
        "\n",
        "print(f\"Iran is encoded as {word_index['iran']} in the data\")\n",
        "print(f\"London is encoded as {word_index['london']} in the data\")\n",
        "print(\"Words are encoded as numbers in our dataset.\")"
      ],
      "execution_count": 65,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Iran is encoded as 779 in the data\n",
            "London is encoded as 544 in the data\n",
            "Words are encoded as numbers in our dataset.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zKRuhDVw-Xee",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "word_index"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GeU0NaXbBnVW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "char_to_int = dict((c, i) for i, c in enumerate(word_index)) # \"enumerate\" retruns index and value. Convert it to dictionary\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BZVNG6HOBr5T",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "test = char_to_int[\"the\"]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9k-PUQAOB6fS",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "96ccdb65-64b9-421e-85e4-73b9e972be07"
      },
      "source": [
        "test"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "9876"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "_QVSlFEAqWJM",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "e23205a4-84f9-44c8-b29b-67374822f0d3"
      },
      "source": [
        "# TODO - your code!\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "num_words = len(word_index) # the number of unique characters\n",
        "\n",
        "print(\"num words : \", num_words)\n"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "num words :  30979\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z828VK-vRUwX",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 467
        },
        "outputId": "5d9e6c2c-3437-40fc-8e3f-cd99658023d9"
      },
      "source": [
        "'''Trains and evaluate a simple MLP\n",
        "on the Reuters newswire topic classification task.\n",
        "'''\n",
        "from __future__ import print_function\n",
        "\n",
        "import numpy as np\n",
        "import keras\n",
        "from keras.datasets import reuters\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Dropout, Activation\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "\n",
        "max_words = 1000\n",
        "batch_size = 32\n",
        "epochs = 5\n",
        "\n",
        "print('Loading data...')\n",
        "(x_train, y_train), (x_test, y_test) = reuters.load_data(num_words=max_words,\n",
        "                                                         test_split=0.2)\n",
        "print(len(x_train), 'train sequences')\n",
        "print(len(x_test), 'test sequences')\n",
        "\n",
        "num_classes = np.max(y_train) + 1\n",
        "print(num_classes, 'classes')\n",
        "\n",
        "print('Vectorizing sequence data...')\n",
        "tokenizer = Tokenizer(num_words=max_words)\n",
        "x_train = tokenizer.sequences_to_matrix(x_train, mode='binary')\n",
        "x_test = tokenizer.sequences_to_matrix(x_test, mode='binary')\n",
        "print('x_train shape:', x_train.shape)\n",
        "print('x_test shape:', x_test.shape)\n",
        "\n",
        "print('Convert class vector to binary class matrix '\n",
        "      '(for use with categorical_crossentropy)')\n",
        "y_train = keras.utils.to_categorical(y_train, num_classes)\n",
        "y_test = keras.utils.to_categorical(y_test, num_classes)\n",
        "print('y_train shape:', y_train.shape)\n",
        "print('y_test shape:', y_test.shape)\n",
        "\n",
        "print('Building model...')\n",
        "model = Sequential()\n",
        "model.add(Embedding(max_features, 128))\n",
        "model.add(LSTM(128, dropout=0.2, recurrent_dropout=0.2))\n",
        "model.add(Dense(num_classes, activation='sigmoid'))\n",
        "\n",
        "model.compile(loss='categorical_crossentropy',\n",
        "              optimizer='adam',\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "history = model.fit(x_train, y_train,\n",
        "                    batch_size=batch_size,\n",
        "                    epochs=epochs,\n",
        "                    verbose=1,\n",
        "                    validation_split=0.1)\n",
        "score = model.evaluate(x_test, y_test,\n",
        "                       batch_size=batch_size, verbose=1)\n",
        "print('Test score:', score[0])\n",
        "print('Test accuracy:', score[1])"
      ],
      "execution_count": 104,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Loading data...\n",
            "8982 train sequences\n",
            "2246 test sequences\n",
            "46 classes\n",
            "Vectorizing sequence data...\n",
            "x_train shape: (8982, 1000)\n",
            "x_test shape: (2246, 1000)\n",
            "Convert class vector to binary class matrix (for use with categorical_crossentropy)\n",
            "y_train shape: (8982, 46)\n",
            "y_test shape: (2246, 46)\n",
            "Building model...\n",
            "Train on 8083 samples, validate on 899 samples\n",
            "Epoch 1/5\n",
            "8083/8083 [==============================] - 455s 56ms/step - loss: 2.5472 - acc: 0.3521 - val_loss: 2.4919 - val_acc: 0.3315\n",
            "Epoch 2/5\n",
            "8083/8083 [==============================] - 430s 53ms/step - loss: 2.4204 - acc: 0.3540 - val_loss: 2.4756 - val_acc: 0.3315\n",
            "Epoch 3/5\n",
            "8083/8083 [==============================] - 432s 53ms/step - loss: 2.4155 - acc: 0.3540 - val_loss: 2.4842 - val_acc: 0.3315\n",
            "Epoch 4/5\n",
            "8083/8083 [==============================] - 434s 54ms/step - loss: 2.4143 - acc: 0.3540 - val_loss: 2.4755 - val_acc: 0.3315\n",
            "Epoch 5/5\n",
            "8083/8083 [==============================] - 431s 53ms/step - loss: 2.4109 - acc: 0.3540 - val_loss: 2.4770 - val_acc: 0.3315\n",
            "2246/2246 [==============================] - 21s 9ms/step\n",
            "Test score: 2.4264975808501137\n",
            "Test accuracy: 0.36197684778237277\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KI0QmLrOMxUV",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 197
        },
        "outputId": "1a49f0ca-29f6-437f-f5ee-f0f341e17501"
      },
      "source": [
        "from keras.preprocessing.text import Tokenizer\n",
        "tokenizer = Tokenizer(num_words=1000)\n",
        "\n",
        "\n",
        "#text = np.array(['this is just some random, stupid text'])\n",
        "\n",
        "#pred = tokenizer.sequences_to_matrix(text, mode='binary')\n",
        "#model.predict(pred)\n",
        "\n",
        "#prediction = model.predict(np.array(tokenizer.texts_to_sequences(text)))\n",
        "#print(prediction)\n",
        "phrase = \"the\"\n",
        "tokens = tokenizer.texts_to_matrix([phrase])\n",
        "\n",
        "model.predict(np.array(tokens))"
      ],
      "execution_count": 105,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0.01100585, 0.12609577, 0.01910624, 0.99958634, 0.66809946,\n",
              "        0.00658533, 0.02209833, 0.00255409, 0.04765695, 0.02246246,\n",
              "        0.03606024, 0.11583731, 0.00796503, 0.05038804, 0.00591469,\n",
              "        0.0039089 , 0.15796852, 0.01482773, 0.02734345, 0.15610713,\n",
              "        0.0939413 , 0.02005816, 0.00278312, 0.00985578, 0.02325255,\n",
              "        0.03407723, 0.00478861, 0.00389019, 0.01112714, 0.00844097,\n",
              "        0.01322705, 0.00837284, 0.00974402, 0.00248024, 0.01616853,\n",
              "        0.00152662, 0.02786419, 0.01012483, 0.00942722, 0.0037736 ,\n",
              "        0.01590324, 0.01200061, 0.0058022 , 0.00322032, 0.00384012,\n",
              "        0.00426057]], dtype=float32)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 105
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3gofqSiobDoe",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 199
        },
        "outputId": "66b10b5f-dc9e-41c1-f718-27e28000dd95"
      },
      "source": [
        "from sklearn.model_selection import GridSearchCV\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense\n",
        "from keras.layers import Dropout\n",
        "from keras.wrappers.scikit_learn import KerasClassifier\n",
        "from keras.constraints import maxnorm\n",
        "\n",
        "\n",
        "def create_model():\n",
        "\t # create model\n",
        "  model = Sequential()\n",
        "  model.add(Embedding(max_features, 128))\n",
        "  model.add(LSTM(128, dropout=0.2, recurrent_dropout=0.2))\n",
        "  model.add(Dense(num_classes, activation='sigmoid'))\n",
        "\n",
        "  model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "  return model\n",
        "# fix random seed for reproducibility\n",
        "seed = 7\n",
        "np.random.seed(seed)\n",
        "# load dataset\n",
        "(x_train, y_train), (x_test, y_test) = reuters.load_data(num_words=max_words,\n",
        "                                                         test_split=0.2)\n",
        "print(len(x_train), 'train sequences')\n",
        "print(len(x_test), 'test sequences')\n",
        "\n",
        "num_classes = np.max(y_train) + 1\n",
        "print(num_classes, 'classes')\n",
        "\n",
        "print('Vectorizing sequence data...')\n",
        "tokenizer = Tokenizer(num_words=max_words)\n",
        "x_train = tokenizer.sequences_to_matrix(x_train, mode='binary')\n",
        "x_test = tokenizer.sequences_to_matrix(x_test, mode='binary')\n",
        "print('x_train shape:', x_train.shape)\n",
        "print('x_test shape:', x_test.shape)\n",
        "\n",
        "print('Convert class vector to binary class matrix '\n",
        "      '(for use with categorical_crossentropy)')\n",
        "y_train = keras.utils.to_categorical(y_train, num_classes)\n",
        "y_test = keras.utils.to_categorical(y_test, num_classes)\n",
        "# create model\n",
        "model = KerasClassifier(build_fn=create_model, verbose=0)\n",
        "# define the grid search parameters\n",
        "batch_size = [10, 20, 40, 60, 80, 100]\n",
        "epochs = [2, 3]\n",
        "\n",
        "\n",
        "param_grid = dict(batch_size=batch_size, epochs=epochs)\n",
        "grid = GridSearchCV(estimator=model, param_grid=param_grid, n_jobs=-1)\n",
        "grid_result = grid.fit(x_train, y_train)\n",
        "# summarize results\n",
        "print(\"Best: %f using %s\" % (grid_result.best_score_, grid_result.best_params_))\n",
        "means = grid_result.cv_results_['mean_test_score']\n",
        "stds = grid_result.cv_results_['std_test_score']\n",
        "params = grid_result.cv_results_['params']\n",
        "for mean, stdev, param in zip(means, stds, params):\n",
        "  print(\"%f (%f) with: %r\" % (mean, stdev, param))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "8982 train sequences\n",
            "2246 test sequences\n",
            "46 classes\n",
            "Vectorizing sequence data...\n",
            "x_train shape: (8982, 1000)\n",
            "x_test shape: (2246, 1000)\n",
            "Convert class vector to binary class matrix (for use with categorical_crossentropy)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/model_selection/_split.py:1978: FutureWarning: The default value of cv will change from 3 to 5 in version 0.22. Specify it explicitly to silence this warning.\n",
            "  warnings.warn(CV_WARNING, FutureWarning)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "lPn6c0x21gu1"
      },
      "source": [
        "Conclusion - RNN runs, and gives pretty decent improvement over a naive model. To *really* improve the model, more playing with parameters would help. Also - RNN may well not be the best approach here, but it is at least a valid one."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "yz0LCZd_O4IG"
      },
      "source": [
        "<a id=\"p2\"></a>\n",
        "## Part 2- CNNs\n",
        "\n",
        "### Find the Frog\n",
        "\n",
        "Time to play \"find the frog!\" Use Keras and ResNet50 (pre-trained) to detect which of the following images contain frogs:\n",
        "\n",
        "<img align=\"left\" src=\"https://d3i6fh83elv35t.cloudfront.net/newshour/app/uploads/2017/03/GettyImages-654745934-1024x687.jpg\" width=400>\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "whIqEWR236Af",
        "outputId": "42bc1784-1f28-4e1c-fd3e-1f77069452f9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 253
        }
      },
      "source": [
        "!pip install google_images_download"
      ],
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting google_images_download\n",
            "  Downloading https://files.pythonhosted.org/packages/18/ed/0319d30c48f3653802da8e6dcfefcea6370157d10d566ef6807cceb5ec4d/google_images_download-2.8.0.tar.gz\n",
            "Collecting selenium (from google_images_download)\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/80/d6/4294f0b4bce4de0abf13e17190289f9d0613b0a44e5dd6a7f5ca98459853/selenium-3.141.0-py2.py3-none-any.whl (904kB)\n",
            "\u001b[K     |████████████████████████████████| 911kB 4.3MB/s \n",
            "\u001b[?25hRequirement already satisfied: urllib3 in /usr/local/lib/python3.6/dist-packages (from selenium->google_images_download) (1.24.3)\n",
            "Building wheels for collected packages: google-images-download\n",
            "  Building wheel for google-images-download (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Stored in directory: /root/.cache/pip/wheels/1f/28/ad/f56e7061e1d2a9a1affe2f9c649c2570cb9198dd24ede0bbab\n",
            "Successfully built google-images-download\n",
            "Installing collected packages: selenium, google-images-download\n",
            "Successfully installed google-images-download-2.8.0 selenium-3.141.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "EKnnnM8k38sN",
        "outputId": "f8de9a59-4e37-48a5-fc17-cda3b3d3ff13",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 703
        }
      },
      "source": [
        "from google_images_download import google_images_download\n",
        "\n",
        "response = google_images_download.googleimagesdownload()\n",
        "arguments = {\"keywords\": \"animal pond\", \"limit\": 15, \"print_urls\": True}\n",
        "absolute_image_paths = response.download(arguments)"
      ],
      "execution_count": 63,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Item no.: 1 --> Item name = animal pond\n",
            "Evaluating...\n",
            "Starting Download...\n",
            "Image URL: https://www.enchantedlearning.com/pgifs/Pondanimals.GIF\n",
            "Completed Image ====> 1.Pondanimals.GIF\n",
            "Image URL: https://i.ytimg.com/vi/NCbu0TND9vE/hqdefault.jpg\n",
            "Completed Image ====> 2.hqdefault.jpg\n",
            "Image URL: https://pklifescience.com/staticfiles/articles/images/PKLS4116_inline.png\n",
            "Completed Image ====> 3.PKLS4116_inline.png\n",
            "Image URL: https://get.pxhere.com/photo/water-animal-pond-wildlife-mammal-fish-eat-fauna-whiskers-vertebrate-otter-mink-marmot-sea-otter-mustelidae-1383482.jpg\n",
            "Completed Image ====> 4.water-animal-pond-wildlife-mammal-fish-eat-fauna-whiskers-vertebrate-otter-mink-marmot-sea-otter-mustelidae-1383482.jpg\n",
            "Image URL: https://pklifescience.com/staticfiles/articles/images/PKLS4116.png\n",
            "Completed Image ====> 5.PKLS4116.png\n",
            "Image URL: https://i.pinimg.com/originals/57/5c/5b/575c5b5c441e27ff04eb50571ee30127.jpg\n",
            "Completed Image ====> 6.575c5b5c441e27ff04eb50571ee30127.jpg\n",
            "Image URL: https://cdn.pixabay.com/photo/2018/04/11/23/05/frog-3312038__340.jpg\n",
            "Completed Image ====> 7.frog-3312038__340.jpg\n",
            "Image URL: https://i.pinimg.com/originals/08/32/22/0832222ee0338d16abf7345aa10bd59f.jpg\n",
            "Completed Image ====> 8.0832222ee0338d16abf7345aa10bd59f.jpg\n",
            "Image URL: https://pixnio.com/free-images/fauna-animals/reptiles-and-amphibians/alligators-and-crocodiles-pictures/alligator-animal-on-pond.jpg\n",
            "Completed Image ====> 9.alligator-animal-on-pond.jpg\n",
            "Image URL: https://www.pixoto.com/images-photography/animals/birds/birds-in-a-pond-5986310798966784.jpg\n",
            "Completed Image ====> 10.birds-in-a-pond-5986310798966784.jpg\n",
            "Image URL: https://cdn.pixabay.com/photo/2017/04/19/20/37/frog-2243543_960_720.jpg\n",
            "Completed Image ====> 11.frog-2243543_960_720.jpg\n",
            "Image URL: https://img-aws.ehowcdn.com/750x428p/photos.demandstudios.com/getty/article/178/192/87827228_XS.jpg\n",
            "Completed Image ====> 12.87827228_XS.jpg\n",
            "Image URL: https://a5.mzstatic.com/us/r30/Purple4/v4/48/05/96/480596cb-892e-df83-830e-76d623bd29fa/screen480x480.jpeg\n",
            "Completed Image ====> 13.screen480x480.jpeg\n",
            "Image URL: http://extension.msstate.edu/sites/default/files/news/extension-outdoors/2016/eo20151113_turtle300.jpg\n",
            "Completed Image ====> 14.eo20151113_turtle300.jpg\n",
            "Image URL: https://cdn.pixabay.com/photo/2017/08/17/06/32/goose-2650209_960_720.jpg\n",
            "Completed Image ====> 15.goose-2650209_960_720.jpg\n",
            "\n",
            "Errors: 0\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "si5YfNqS50QU"
      },
      "source": [
        "At time of writing at least a few do, but since the Internet changes - it is possible your 5 won't. You can easily verify yourself, and (once you have working code) increase the number of images you pull to be more sure of getting a frog. Your goal is to validly run ResNet50 on the input images - don't worry about tuning or improving the model.\n",
        "\n",
        "*Hint* - ResNet 50 doesn't just return \"frog\". The three labels it has for frogs are: `bullfrog, tree frog, tailed frog`\n",
        "\n",
        "*Stretch goal* - also check for fish."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "FaT07ddW3nHz",
        "colab": {}
      },
      "source": [
        "# TODO - your code!import numpy as np\n",
        "\n",
        "from keras.applications.resnet50 import ResNet50\n",
        "from keras.preprocessing import image\n",
        "from keras.applications.resnet50 import preprocess_input, decode_predictions\n",
        "import requests\n",
        "\n",
        "def process_img_path(img_path):\n",
        "  return image.load_img(img_path, target_size=(224, 224))\n",
        "\n",
        "def img_contains_frog(img):\n",
        "  x = image.img_to_array(img)\n",
        "  x = np.expand_dims(x, axis=0)\n",
        "  x = preprocess_input(x)\n",
        "  model = ResNet50(weights='imagenet')\n",
        "  features = model.predict(x)\n",
        "  results = decode_predictions(features, top=3)[0]\n",
        "  print(results)\n",
        "  for entry in results:\n",
        "    if entry[1] == 'bullfrog' or entry[1] == 'tree frog' or entry[1] == 'tailed frog':\n",
        "      return entry[2]\n",
        "  return 0.0"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jwiPmm2NHFrx",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 287
        },
        "outputId": "2e376644-2462-4f86-eca8-e1f7fc9e0803"
      },
      "source": [
        "for path in absolute_image_paths[0]['animal pond']:\n",
        "  img_contains_frog(process_img_path(path))"
      ],
      "execution_count": 64,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[('n03598930', 'jigsaw_puzzle', 0.8680324), ('n06359193', 'web_site', 0.064099774), ('n02834397', 'bib', 0.021264149)]\n",
            "[('n01443537', 'goldfish', 0.8495884), ('n01631663', 'eft', 0.067601345), ('n02536864', 'coho', 0.03516318)]\n",
            "[('n04243546', 'slot', 0.8712438), ('n04476259', 'tray', 0.04993648), ('n03908618', 'pencil_box', 0.023072599)]\n",
            "[('n02442845', 'mink', 0.30976573), ('n02363005', 'beaver', 0.2339894), ('n02361337', 'marmot', 0.20796825)]\n",
            "[('n03485794', 'handkerchief', 0.88227284), ('n02834397', 'bib', 0.02268081), ('n03291819', 'envelope', 0.020095214)]\n",
            "[('n02116738', 'African_hunting_dog', 0.610158), ('n02105162', 'malinois', 0.19866747), ('n02114712', 'red_wolf', 0.051152844)]\n",
            "[('n01737021', 'water_snake', 0.30730632), ('n01641577', 'bullfrog', 0.26061353), ('n04275548', 'spider_web', 0.11343399)]\n",
            "[('n06359193', 'web_site', 0.5092322), ('n02840245', 'binder', 0.10986997), ('n04209239', 'shower_curtain', 0.100119166)]\n",
            "[('n01698640', 'American_alligator', 0.96394145), ('n01697457', 'African_crocodile', 0.026759788), ('n01737021', 'water_snake', 0.005964627)]\n",
            "[('n02009912', 'American_egret', 0.7822409), ('n02012849', 'crane', 0.14339265), ('n02009229', 'little_blue_heron', 0.021143379)]\n",
            "[('n01641577', 'bullfrog', 0.9223302), ('n01644900', 'tailed_frog', 0.07364718), ('n01644373', 'tree_frog', 0.0011781217)]\n",
            "[('n01443537', 'goldfish', 0.95694786), ('n01833805', 'hummingbird', 0.009290858), ('n12620546', 'hip', 0.0034554903)]\n",
            "[('n03710721', 'maillot', 0.17273408), ('n04371430', 'swimming_trunks', 0.12428097), ('n04476259', 'tray', 0.09713087)]\n",
            "[('n01667778', 'terrapin', 0.4962086), ('n01667114', 'mud_turtle', 0.25764626), ('n01669191', 'box_turtle', 0.11115054)]\n",
            "[('n01860187', 'black_swan', 0.8796096), ('n01847000', 'drake', 0.0339848), ('n01855032', 'red-breasted_merganser', 0.028970907)]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KNMUzQsNmP1b",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# TODO - your code!import numpy as np\n",
        "\n",
        "from keras.applications.resnet50 import ResNet50\n",
        "from keras.preprocessing import image\n",
        "from keras.applications.resnet50 import preprocess_input, decode_predictions\n",
        "import requests\n",
        "\n",
        "def process_img_path(img_path):\n",
        "  return image.load_img(img_path, target_size=(224, 224))\n",
        "\n",
        "def img_contains_fish(img):\n",
        "  x = image.img_to_array(img)\n",
        "  x = np.expand_dims(x, axis=0)\n",
        "  x = preprocess_input(x)\n",
        "  model = ResNet50(weights='imagenet')\n",
        "  features = model.predict(x)\n",
        "  results = decode_predictions(features, top=3)[0]\n",
        "  print(results)\n",
        "  for entry in results:\n",
        "    if entry[1].contains('fish'):\n",
        "      return entry[2]\n",
        "  return 0.0"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "XEuhvSu7O5Rf"
      },
      "source": [
        "<a id=\"p3\"></a>\n",
        "## Part 3 - Generative Adverserial Networks (GANS)\n",
        "\n",
        "Describe the difference between a discriminator and generator in a GAN in your own words.\n",
        "\n",
        "The job of the generator is to generate an image that looks real. The job of the discriminator is to give a probability that the generated image is real.\n",
        "\n",
        "A well trained GAN finds the generator and discriminator in a Nash equilibrium."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "626zYgjkO7Vq"
      },
      "source": [
        "<a id=\"p4\"></a>\n",
        "## Part 4 - More..."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "__lDWfcUO8oo"
      },
      "source": [
        "Answer the following questions, with a target audience of a fellow Data Scientist:\n",
        "\n",
        "- What do you consider your strongest area, as a Data Scientist?\n",
        "\n",
        "I've been told several times that I'm good at explaining technical things to non-technical people. This is a good skill for a data scientist, as we will often be required to explain analyses of data to non-technical people in a non-technical but detailed manner. \n",
        "\n",
        "- What area of Data Science would you most like to learn more about, and why?\n",
        "\n",
        "I'd like to learn more about dealing with \"big data\"—particularly when that data is unstructured. Data in the real world is often messy, but valuable. I think my skills need sharpening in this area. Thankfully, we'll be having more instruction on this.\n",
        "\n",
        "- Where do you think Data Science will be in 5 years?\n",
        "\n",
        "It looks as though automl will be very effective in five years or so. Google is making very nice progress here. I expect that more people will have a decent understanding of what exactly it is that data scientists do and when to hire them. Perhaps at this point we will see more technical advancements? After I have more experience \"in the field\" the answer to this question should be clearer to me. \n",
        "\n",
        "- What are the treats posed by AI to our society?\n",
        "\n",
        "Job displacement is the most immediate threat. One could easily imagine mass outrage leading to politicians campaigning on AI-regulation. If we don't accidentally create harmful AGI and manage to create it in a manner that allows it to augment human intelligence, society will fundamentally change on a massive scale. This is increasingly a bad thing. However, this is a distant threat and I rarely think of it. Too many unknowns.\n",
        "\n",
        "- How do you think we can counteract those threats? \n",
        "\n",
        "Regulation is a line of dominoes and China would be the only winner in this case. Don't let the government have any say in who gets to develop AI. Focus more on AI-security startups. \n",
        "\n",
        "- Do you think achieving General Artifical Intelligence is ever possible?\n",
        "\n",
        "Yes, but probably not before 2050. There's not a strong commercial incentive behind AGI research right now and consciousnesss seems to become more mystifying the more we learn about it.\n",
        "\n",
        "A few sentences per answer is fine - only elaborate if time allows."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "_Hoqe3mM_Mtc"
      },
      "source": [
        "## Congratulations! \n",
        "\n",
        "Thank you for your hard work, and congratulations! You've learned a lot, and you should proudly call yourself a Data Scientist.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NJarOcum5kCc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from IPython.display import HTML\n",
        "\n",
        "HTML(\"\"\"<iframe src=\"https://giphy.com/embed/26xivLqkv86uJzqWk\" width=\"480\" height=\"270\" frameBorder=\"0\" class=\"giphy-embed\" allowFullScreen></iframe><p><a href=\"https://giphy.com/gifs/mumm-champagne-saber-26xivLqkv86uJzqWk\">via GIPHY</a></p>\"\"\")"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}